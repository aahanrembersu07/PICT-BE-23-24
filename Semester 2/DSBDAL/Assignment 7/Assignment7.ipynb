{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2884875b-8476-4a87-b7fd-14a9f91ee61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neils\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef79a512-a48a-4d15-b940-b3a3e2eedc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadFromFile(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5238929b-0863-452e-9ffe-d8b030315ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=ReadFromFile('sample1.txt')\n",
    "file2=ReadFromFile('sample2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7c20ce-100c-4de2-bf5b-edfdb99a5af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achieve', 'achieve', 'academic', 'excellence', 'by', 'imparting', 'education', 'in', 'computing', 'and', 'research', 'hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def Tokenize(sentence:str):\n",
    "    punctuation=string.punctuation+'[]{}()<>'\n",
    "    for char in punctuation:\n",
    "        sentence=sentence.replace(char,\" \")\n",
    "    sentence=sentence.lower()\n",
    "    tokens=sentence.split()\n",
    "    return tokens\n",
    "tokens=Tokenize(file1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab08d6e5-944d-455e-b19b-b87e6e3d2343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achieve', 'achieve', 'academic', 'excellence', 'by', 'imparting', 'education', 'in', 'computing', 'and', 'research', 'hello', 'world']\n",
      "['hello', 'hello', 'world', 'pict', 'abcd']\n"
     ]
    }
   ],
   "source": [
    "tokens1=Tokenize(file1)\n",
    "tokens2=Tokenize(file2)\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c195e3-94df-4bb1-bc4e-a2fe1a02ad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achieve', 'achieve', 'academic', 'excellence', 'imparting', 'education', 'computing', 'research', 'hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "def RemoveStopWords(tokens):\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filtered_sentence=[word for word in tokens if word not in stop_words]\n",
    "    return filtered_sentence\n",
    "tokens=RemoveStopWords(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6511b0fb-2ae0-4db9-8615-4e5e8fb58032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\neils\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0ecb78-420d-4df9-b328-2a4e82df080c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('achieve', 'NNS'),\n",
       " ('achieve', 'VBP'),\n",
       " ('academic', 'JJ'),\n",
       " ('excellence', 'NN'),\n",
       " ('imparting', 'VBG'),\n",
       " ('education', 'NN'),\n",
       " ('computing', 'VBG'),\n",
       " ('research', 'NN'),\n",
       " ('hello', 'NN'),\n",
       " ('world', 'NN')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_list=pos_tag(tokens)\n",
    "pos_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c42a171-ce85-4410-be01-8b700450fd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming Words\n",
      "achieve : achiev \n",
      "achieve : achiev \n",
      "academic : academ \n",
      "excellence : excel \n",
      "imparting : impart \n",
      "education : educ \n",
      "computing : comput \n",
      "research : research \n",
      "hello : hello \n",
      "world : world \n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "print(\"Stemming Words\")\n",
    "for w in tokens:\n",
    "    print(f\"{w} : {stemmer.stem(w)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87fff1e1-7120-481e-93a9-04e8790c3a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\neils\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c8549e-2eef-4c1c-b82b-9c10f7b1a635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\neils\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a84c0c4c-b624-4978-8b41-5362fe9518e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "achieve : achieve \n",
      "achieve : achieve \n",
      "academic : academic \n",
      "excellence : excellence \n",
      "imparting : imparting \n",
      "education : education \n",
      "computing : computing \n",
      "research : research \n",
      "hello : hello \n",
      "world : world \n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "for w in tokens:\n",
    "    print(f\"{w} : {lemmatizer.lemmatize(w)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc8e59b3-2584-4ea8-9db6-8b57c370ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateTermFrequency(tokens):\n",
    "    term_freq={}\n",
    "    for word in tokens:\n",
    "        if word not in term_freq:\n",
    "            term_freq[word]=tokens.count(word)/len(tokens)\n",
    "    return term_freq\n",
    "tf1=CalculateTermFrequency(tokens1)\n",
    "tf2=CalculateTermFrequency(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69de8db2-7bed-4345-b8e7-26c694b79ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 0.6931471805599453, 'computing': 0.6931471805599453, 'research': 0.6931471805599453, 'achieve': 0.6931471805599453, 'academic': 0.6931471805599453, 'in': 0.6931471805599453, 'education': 0.6931471805599453, 'world': 0.0, 'excellence': 0.6931471805599453, 'and': 0.6931471805599453, 'imparting': 0.6931471805599453, 'hello': 0.0, 'pict': 0.6931471805599453, 'abcd': 0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "d1_tokens=Tokenize(file1)\n",
    "d2_tokens=Tokenize(file2)\n",
    "total_documents=2\n",
    "word_in_doc={}\n",
    "for tokens in [d1_tokens,d2_tokens]:\n",
    "    for word in set(tokens):\n",
    "        word_in_doc[word]=word_in_doc.get(word,0)+1\n",
    "idf={}\n",
    "for word in word_in_doc:\n",
    "    idf[word]=math.log(total_documents/word_in_doc[word])\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a887faa8-408d-4e30-99f0-8c1a832a6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_repr=[]\n",
    "for token in d1_tokens:\n",
    "    d1_repr.append(tf1[token]*idf[token])\n",
    "d2_repr=[]\n",
    "for token in d2_tokens:\n",
    "    d2_repr.append(tf2[token]*idf[token])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4b5a7a4-c445-4b90-9343-4716639725dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10663802777845313, 0.10663802777845313, 0.053319013889226566, 0.053319013889226566, 0.053319013889226566, 0.053319013889226566, 0.053319013889226566, 0.053319013889226566, 0.053319013889226566, 0.053319013889226566, 0.053319013889226566, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(d1_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a272a6c8-6838-481a-906f-7a9e708b18f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905]\n"
     ]
    }
   ],
   "source": [
    "print(d2_repr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
